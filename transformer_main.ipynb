{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Representations of Gene Ontology: Embedding Term Descriptors using Large Language Models (LLMs)\n",
    "\n",
    "The Gene Ontology (GO) is a directed knowledge graph where each node represents a specific GO term, defining a particular aspect of genes and their products. These GO terms are categoryzed into three primary ontologies:\n",
    "\n",
    "- <font color=\"grey\">***Molecular Function (MF)***</font>: These terms define the **activities** performed by gene products, such as *catalysis* or \"*transport*\", etc. These functions can be refined by more more specific GO terms, such as \"protein kinase activity\" within the broader category of \"catalysis\"\n",
    "\n",
    "\n",
    "- <font color=\"grey\">***Cellular Component (CC)***</font>: These terms specify the subcellular **location** of gene products, emcompasing compartments like *chloroplast* or *nucleus* , as well as macromolecular complexes like *proteasome* or *ribosome*\n",
    "\n",
    "\n",
    "- <font color=\"grey\">***Biological Process (BP)***</font>: These terms delineate the biological **pathways** in which gene products are involved, ranging from 'DNA repair' and 'carbohydrate metabolic process' to more overaching processes like *biosynthetic processes*\n",
    "\n",
    "The GO graph integrates these these three aspects of gene annotation, presenting a **hierarchical** organization with **interconnections denoted by edges representing the relationships between terms**. Consequently, the GO graph can be visualized as three distinct but connected trees. However, bear in mind that the three GO ontologies are *'is a'* disjoint because there is no such relationship between terms from different trees, but other types of connections like *'part of'* and *'regulates'* exist. We also define terms as *parents* and *children* based on their position in the tree that we can loosely call *generations*: terms closer to the root are parents, while those closer to the terminal leaves are child terms. Please check the [Gene Ontology reference](https://geneontology.org/docs/ontology-documentation) for more information about how the GO graph is structured.\n",
    "\n",
    "The GO graph empower us to classify genes and proteins based on their functions in associated processes ocurring in a specific subcellular compartment, being an essential tool to predict functions of newly discovered genes. One feature often overlooked when deploying the GO graph to assist prediction and classification of gene function is the **textual definition of each GO term**. To harness the power of this valuable GO graph content, we will apply **Natural Language Processing** (**NLP**) tecniques to analyze and interpret these statements.\n",
    "\n",
    "\n",
    ">**Natural Language Processing (NLP)** is a burgeoning field in machine learning, focusing on techniques for extracting contextualized information from word sequences. It finds applications in diverse areas, including sentiment analysis and text generation for product reviews and chatbot interactions.\n",
    "\n",
    "\n",
    "In this tutorial, we will use the text definitions of GO terms to fine-tune a pretrained Natural Language Processing (NLP) Large Language Model (LLMs). Fine-tunning is a powerful tecnique in machine learning that consists adapting models pre-trained on extensive textual data from different sources to specific applications. This task is done by keeping the deeper encoded features of the pre-trained model but adjusting the trainable parameters of the input and output layers. To demonstrate the power of this approach, we will conduct an analysis of Molecular Biology articles and try to accuratelly identify which primary ontology (BP, MF or CC) best characterizes the paper. This practical exercise will highlight the practicality of fine-tuning within a real-world context. We will also use the fine-tunned model to embed the text definitions into feature vectors to enrich nodes' features in the GO graph. By adding this additional attribute to the nodes, we aim to improve the power of GO graph in assisting machine learning applications such as prediction of gene functions and the outcome of metabolic pathways.\n",
    "\n",
    "\n",
    ">**Text Embeddings**:  Machine learning algorithms primarily operate on numerical data. To utilize text data in these models, it must be transformed into numerical representations. Text embeddings are techniques that extract meaningful hidden features from text while preserving the sequential relationships between words. This ensures that the essential meaning of the text is preserved in the feature vector. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required libraries\n",
    "\n",
    "# !pip install obonet\n",
    "# !pip install nltk\n",
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# !pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from obonet import read_obo\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "from wordcloud import WordCloud\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "# progress bar for loops\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "# set home directory\n",
    "from pathlib import Path\n",
    "\n",
    "home_dir = Path(\"../GO-graph-embed/\")\n",
    "print(f\"Home directory: {home_dir.as_posix()}\")\n",
    "\n",
    "# download / update punkt, the nltk's recommended sentences' tokenizer\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# custom functions\n",
    "\n",
    "# plot confusion matrix\n",
    "def plot_cm(y_true: pd.Series, y_pred: pd.Series):\n",
    "    # print out the balanced accuracy score\n",
    "    print(f\"Balanced Accuracy Score: {balanced_accuracy_score(y_pred, y_true):.1%}\")\n",
    "\n",
    "    cm = confusion_matrix(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        normalize=\"pred\",\n",
    "    )\n",
    "\n",
    "    # converting into dataframe for easy plotting\n",
    "    cm_df = pd.DataFrame(\n",
    "        cm,\n",
    "        index=[\"Biological Process\", \"Cellular Component\", \"Molecular Function\"],\n",
    "        columns=[\"Biological Process\", \"Cellular Component\", \"Molecular Function\"],\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.set(font_scale=1.2)\n",
    "    sns.heatmap(\n",
    "        data=cm_df,\n",
    "        annot=True,\n",
    "        cmap=\"Blues\",\n",
    "    )\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Parsing GO Graph Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File format\n",
    "\n",
    "The GO graph is stored in the *OBO (Open Biomedical Ontologies) file oboformat*, which is specifically tailored for constructing and representing biological ontologies. We can use the Python library *obonet* to read and parse the *obo* file to convert it into a **NetworkX** object. [NetworkX](https://networkx.org/) provides a comprehensive framework to manipulate and analyze graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_graph = read_obo(home_dir.joinpath(\"data/go-basic.obo\"))\n",
    "\n",
    "print(f\"The GO graph is a {go_graph}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node attributes\n",
    "\n",
    "The nodes contain attributes describing the respective GO terms. let's print out the attributes of a randomly selected node to get a glimpse these attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_graph.nodes()[random.choice([x for x in go_graph.nodes()])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All GO terms contain the following essential attributes:\n",
    "\n",
    ">***name***: unique identifier of the term in a human-readable format\n",
    "\n",
    ">***namespace***: one of the three major ontologies (MF, CC or BP) to which the term belongs\n",
    "\n",
    ">***definition***: short description of what the GO term means for humans. It can also contains references to publications defining the term (e.g. PMID:10873824).\n",
    "\n",
    "There are additional attributes of each node corresponding to GO terms, but we won't be using them in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Dataframe with Nodes' attributes\n",
    "\n",
    "We will extract the attributes from GO graph nodes into a dataframe to facilitate the processing of the text definitions. We will also add a column with the length of the text definition of each GO term. This will help us to determine the parameters when deploying **pre-trained Large Language Models (LLMs)** to extract features from GO terms' definitions and embed them into a feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create GO definitions dataframe\n",
    "\n",
    "# empty dataframe to store nodes' attributes\n",
    "go_df = pd.DataFrame(columns=[\"go_id\", \"name\", \"aspect\", \"definition\", \"length\"])\n",
    "\n",
    "# iterating over nodes to extract dictionary keys and values\n",
    "for idx, item in tqdm_notebook(\n",
    "    enumerate(go_graph.nodes.items()), total=len(go_graph.nodes)\n",
    "):\n",
    "    go_term = item[0]\n",
    "    name = item[1][\"name\"]\n",
    "    aspect = item[1][\"namespace\"]\n",
    "    definition = item[1][\"def\"].split(sep='\"', maxsplit=2)[1]\n",
    "    length = len(definition)\n",
    "    go_df.loc[idx] = [\n",
    "        go_term,\n",
    "        name,\n",
    "        aspect,\n",
    "        definition,\n",
    "        length,\n",
    "    ]\n",
    "\n",
    "# saving dataframe\n",
    "home_dir.joinpath(\"data/\").mkdir(parents=True, exist_ok=True)\n",
    "go_df.to_csv(home_dir.joinpath(\"data/go_df.csv\"), index=False)\n",
    "\n",
    "go_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading saved dataframe with GO graph nodes' attributes\n",
    "\n",
    "go_df = pd.read_csv(home_dir.joinpath(\"data/go_df.csv\"))\n",
    "\n",
    "print(f\"GO entries: {go_df.shape[0]}, features: {go_df.shape[1]}\")\n",
    "display(go_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visually inspect the length (the **number of characters** in this case...) distribution of GO terms text definitions. The [`seaborn.histplot()`](https://seaborn.pydata.org/generated/seaborn.histplot.html) function can plot the *kernel density distribution* (*KDE*) to estimate the text length distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_hist = sns.histplot(go_df[\"length\"].values, bins=100, kde=True)\n",
    "go_hist.set_title(\"GO definition lengths\")\n",
    "go_hist.set_xlabel(\"Definition Lengths\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the length of the GO terms definitions and analyzing the quartiles, we can see that most definitions fall bellow 200 words (75%), with mean length of ~150 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Trained Transformer Language Models (LLMs)\n",
    "\n",
    "[**Transformers**](https://huggingface.co/docs/transformers/index) for text classification is a 'hot' topic in machine learning and behind hipped generative models like [chatGPT](https://chat.openai.com). Text transformer architectures are designed to find dependencies between words in a sentence regarding their positions and use these dependencies for text ranking or classification, in addition to generative models capable of generate new sentences based on a query.\n",
    "\n",
    "We will **fine-tune** (i.e. train the model to perform a **specific task**) the pretrained **Large Language Model** (LLM) [bert-base-multilingual-cased](https://huggingface.co/bert-base-multilingual-cased) to classify GO term definitions regarding the major GO ontologies (BP, CC and MF) to which each one belong. We will also obtain text embeddings of GO term definitions to use them as additional data to train models to predict protein annotations.\n",
    "\n",
    "The model is the  based on the [Bidirectional Encoder Representations from Transformers (BERT)](https://huggingface.co/docs/transformers/model_doc/bert) architecture pre-trained with millions of [Wikipedia](https://www.wikipedia.org/) entries in 104 languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Datasets\n",
    "\n",
    "We need to preprocess the dataset in such a way that the pre-trained model can understand. First we'll create train, validation and test datasets with the input text from GO terms' definitions. Then, the datasets will be **'tokenized'**, i.e., the input texts will be splitted into tokens using a set of rules and delimited by special tokens that says to the model where the sequence and sentences start and end.\n",
    "\n",
    " The datasets are created using the Hugging Face's library [Dataset](https://huggingface.co/docs/datasets/index). Actually, later we will be using pre-trained sequence classification models and respective tokenizers from [Hugging Face](https://huggingface.co/).\n",
    "\n",
    "Before that, we change the *definition* column name to *text* and change the *aspect* column type to categorical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename definition column\n",
    "go_df.rename(columns={\"definition\": \"text\"}, inplace=True)\n",
    "\n",
    "# converting categorical labels to numbers (aspect)\n",
    "go_df[\"aspect\"] = pd.Categorical(go_df[\"aspect\"])\n",
    "\n",
    "# getting categorical codes\n",
    "go_df[\"label\"] = go_df[\"aspect\"].cat.codes\n",
    "\n",
    "display(go_df)\n",
    "print(go_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate over the aspect categories we just created to check the correspondence between the classes and the label numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for code, aspect in enumerate(go_df.aspect.cat.categories):\n",
    "    print(f\"{code}: {aspect}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting only the columns we need for model's fine-tunning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = go_df[[\"text\", \"label\"]].copy()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the most frequent terms in the data by plotting a *word cloud*, which is a nice way to grasp the most frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_wordcloud = WordCloud(\n",
    "    width=800, height=800, background_color=\"white\", min_font_size=10, colormap=\"tab20b\",\n",
    ").generate(data.text.to_string())\n",
    "\n",
    "plt.figure(figsize=(8, 8), facecolor=None)\n",
    "plt.suptitle(\"Word cloud of GO term definitions\")\n",
    "plt.imshow(go_wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of characters of each definition is showed in the *def_length* column. The language model that we'll fine-tune and use for predictions accepts a limited number of tokens as input In our case study, the **bert-base-multilingual-cased** accepts **512 tokens** (actually 500 tokens and two special tokens [CLS] and [SEP] that we will get to know later).\n",
    "\n",
    "A box-plot can provide useful insight on the lengths to choose the better hyperparameters for tokenization and training. Here, we use the **re** module to find all the words in each definition and return the length using a **lambda** function applied to the *definition* column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(2, 4))\n",
    "plt.title(\"Dataset Word Count\")\n",
    "\n",
    "plt.boxplot(data.text.apply(lambda x: len(re.findall(r\"\\w+\", x))))\n",
    "plt.ylabel(\"Word Count\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All GO terms' definitions fit the input size required for fine-tuning (510 tokens) but we may want to limit the length of inputs to speed-up the fine-tuning process. Let's proceed to create the dataset with train and test sets using Hugging Face's Dataset `train_test_split()` method. We use stratification to preserve labels ratios between datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# create dataset\n",
    "dataset = Dataset.from_pandas(data)\n",
    "\n",
    "# change label column to ClassLabe to allow stratification\n",
    "dataset = dataset.class_encode_column(\"label\")\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.3, stratify_by_column=\"label\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Tokenization strategy must match the model of choice**. This is key to ensure that tokens will be mapped to the **same indices** presented to the model during the training phase and also use the **same special tokens** to delimit the beggining of a text sequence and the separation between sentences.\n",
    "\n",
    "Here, we use [BertTokenizerFast](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertTokenizerFast) using the tokenization strategy of the pre-trained model (bert-base-multilingual-uncased) to get rid of uppercase and capital letters. With lowercase inputs, the number of tokens is usually smaller and generalizes better for unseen text sequences inputs in **production phase**. Moreover, we can change it later and see if the model performs better or not with cased inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, max_length=150, truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaining the keys in train_ds sample items:\n",
    "\n",
    "- **input_ids**: indices of each token in the sequence (these indices were generated during model pre-training)\n",
    "- **token_type_ids**:  references of the sentence to which the token belongs\n",
    "- **attention_mask**: indicates if the token should be attended or not\n",
    "\n",
    "We can decode the the **input_ids** to obtain the original text. Below we print out the decoded definitions based on the input_ids (I did some changes in the print for better visualization of the decoded input ids and the input ids):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_len = len(tokenized_dataset[\"train\"][\"input_ids\"])\n",
    "\n",
    "for i in range(5):\n",
    "    idx = random.randint(0, dataset_len)\n",
    "    print(tokenizer.decode(tokenized_dataset[\"train\"][\"input_ids\"][idx]))\n",
    "    input_ids_list = tokenized_dataset[\"train\"][\"input_ids\"][idx]\n",
    "    print(input_ids_list)\n",
    "    print(f\"Input length: {len(input_ids_list)}\")\n",
    "    print(50 * \"=\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output above we can see the sentences delimited by the special tokens **Classification Token [CLS]** and **Separator Token [SEP]**. [CLS] delimits the start of the sequence for the BERT model we will be using. \n",
    "\n",
    "and set the model to generate embeddings for sequence classification tasks. [SEP] is used to separate sentences in sentence-pair tasks and help the model to capture the relationships between the two senteces concatenated by the [SEP]. The **padding to ensure same-sized sentences** during tokenization is represented by the special padding token **[PAD]**.\n",
    "\n",
    ">**Truncation and Padding**: truncation and padding parameters tell to the tokenizer how to handle sequences of variable length. Input sequences with more tokens than the number accepted by the model will be truncated if *truncation* parameter is 'True'. On the other hand, if the sequences are shorter than the model's accept, the special padding token [PAD] will be added until the number of tokens in the input match the model's requireiments. *Truncation* and *padding* are key to ensure that the input will be the same size. This is key because the input of the model are fixed-size tensors set during model training (510 + 2 special tokens [CLS] and [SEP] = 512).\n",
    "\n",
    "Now, let's instantiate the model class [BertForSequenceClassification](https://huggingface.co/docs/transformers/v4.33.2/en/model_doc/bert#transformers.BertForSequenceClassification) loaded with the **weights** from the pre-trained model \"bert-base-multilingual-cased\" and define the number of target labels we want to predicted. In our case-study, we will predict if the text talks about Biological Process, Cellular Component or Molecular Function. Thus, the number of labels is **3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"cuda available?: {torch.cuda.is_available()}\")\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-uncased\", num_labels=3)\n",
    "\n",
    "model.cuda()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fine-tune the model using the [Trainer](https://huggingface.co/docs/transformers/v4.33.2/en/main_classes/trainer#transformers.Trainer) class optimized to train Hugging Face's models.\n",
    "\n",
    "We need to configure the **trainer's hyperparameters** using the [TrainingArguments](https://huggingface.co/docs/transformers/v4.33.2/en/main_classes/trainer#transformers.TrainingArguments) and set the evaluation metrics using the [evaluate](https://huggingface.co/docs/evaluate/index) module to monitor the training process and evaluate the performance after fine-tuning.\n",
    "\n",
    ">To see the full list of tunable training hyperparameters and available metrics, click the links above. Alternativelly, you can make some elegant code to show them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Trainer\n",
    "from transformers import TrainingArguments\n",
    "import accelerate\n",
    "\n",
    "train_args = TrainingArguments(output_dir=\"test_trainer\")\n",
    "\n",
    "# list training parameters\n",
    "args_list = [\n",
    "    x for x in list(dir(train_args)) if not re.compile(r\"\\b_[A-Za-z0-9_]*\\b\").match(x)\n",
    "]\n",
    "print(f\"There are {len(args_list)} training arguments: {args_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Trainer does not have an automatic evaluator of model's performance during training, we have to add metrics using *evaluate* and pass it to Trainer through TrainingArguments.\n",
    "\n",
    "We can use ROC/AUC score to assess model's performance. Additionally, we need to define a function to convert the predictions to logits to pass it to the *compute* method as explained by [this tutorial](https://huggingface.co/docs/transformers/training) in Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# list metrics\n",
    "metrics_list = evaluate.list_evaluation_modules()\n",
    "print(f\"There are {len(metrics_list)} available metrics: {metrics_list}\")\n",
    "\n",
    "# load roc_auc metric\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "# convert preds --> logits\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "# set evaluation for training\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(10000))\n",
    "small_eval_dataset = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(2000))\n",
    "\n",
    "print(small_train_dataset)\n",
    "print(small_eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tunning in Native PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
